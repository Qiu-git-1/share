"""Anomaly metrics."""
import numpy as np
from sklearn import metrics

'''
This code contains two main functions to calculate anomaly detection performance metrics for image retrieval and pixel-level retrieval. These indicators include the area under the ROC curve (AUROC), true case rate (TPR), false positive case rate (FPR), and other relevant statistics. Here is a detailed description of each function:

1.compute_imagewise_retrieval_metrics(anomaly_prediction_weights, anomaly_ground_truth_labels):
This function is used to calculate image-level retrieval metrics. It accepts two parameters, anomaly_prediction_weights being the fraction of each image predicted by the model to be abnormal, and anomaly_ground_truth_labels being the true labels of the images, where 1 means abnormal and 0 means normal.

The sklearn.metrics.roc_curve is used internally to calculate the area under the ROC curve (AUROC), as well as the true case rate (TPR) and false positive case rate (FPR). These values can be used to plot an ROC curve, and the higher the AUROC value, the better the performance of the model in distinguishing between abnormal and normal images.

2.compute_pixelwise_retrieval_metrics(anomaly_segmentations, ground_truth_masks):
This function is used to compute pixel level retrieval metrics. It takes two arguments, anomaly_segmentations being the list of exception segmentation masks generated by the model, and ground_truth_masks being the list of actual exception segmentation masks.

First, the function flattens the mask list into a one-dimensional array to make it easier to compute metrics. Then use sklearn.metrics.roc_curve and sklearn.metrics.roc_auc_score to calculate the AUROC and ROC curve. In addition, the Precision-Recall Curve was calculated, along with the F1 score, in order to find the optimal threshold and calculate the corresponding FPR and FNR.

Finally, the function returns a dictionary containing AUROC, FPR, TPR, the best threshold, and the best FPR and FNR. These indexes can help evaluate the anomaly detection performance of the model at the pixel level.

These two functions are very useful for evaluating the performance of an anomaly detection model, especially when we focus on whether the entire image is an anomaly or whether a specific area in the image is an anomaly.
'''
def compute_imagewise_retrieval_metrics(
    anomaly_prediction_weights, anomaly_ground_truth_labels
):
    """
    Computes retrieval statistics (AUROC, FPR, TPR).

    Args:
        anomaly_prediction_weights: [np.array or list] [N] Assignment weights
                                    per image. Higher indicates higher
                                    probability of being an anomaly.
        anomaly_ground_truth_labels: [np.array or list] [N] Binary labels - 1
                                    if image is an anomaly, 0 if not.
    """
    fpr, tpr, thresholds = metrics.roc_curve(
        anomaly_ground_truth_labels, anomaly_prediction_weights
    )
    auroc = metrics.roc_auc_score(
        anomaly_ground_truth_labels, anomaly_prediction_weights
    )
    return {"auroc": auroc, "fpr": fpr, "tpr": tpr, "threshold": thresholds}


def compute_pixelwise_retrieval_metrics(anomaly_segmentations, ground_truth_masks):
    """
    Computes pixel-wise statistics (AUROC, FPR, TPR) for anomaly segmentations
    and ground truth segmentation masks.

    Args:
        anomaly_segmentations: [list of np.arrays or np.array] [NxHxW] Contains
                                generated segmentation masks.
        ground_truth_masks: [list of np.arrays or np.array] [NxHxW] Contains
                            predefined ground truth segmentation masks
    """
    if isinstance(anomaly_segmentations, list):
        anomaly_segmentations = np.stack(anomaly_segmentations)
    if isinstance(ground_truth_masks, list):
        ground_truth_masks = np.stack(ground_truth_masks)

    flat_anomaly_segmentations = anomaly_segmentations.ravel()
    flat_ground_truth_masks = ground_truth_masks.ravel()

    fpr, tpr, thresholds = metrics.roc_curve(
        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations
    )
    auroc = metrics.roc_auc_score(
        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations
    )

    precision, recall, thresholds = metrics.precision_recall_curve(
        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations
    )
    F1_scores = np.divide(
        2 * precision * recall,
        precision + recall,
        out=np.zeros_like(precision),
        where=(precision + recall) != 0,
    )

    optimal_threshold = thresholds[np.argmax(F1_scores)]
    predictions = (flat_anomaly_segmentations >= optimal_threshold).astype(int)
    fpr_optim = np.mean(predictions > flat_ground_truth_masks)
    fnr_optim = np.mean(predictions < flat_ground_truth_masks)

    return {
        "auroc": auroc,
        "fpr": fpr,
        "tpr": tpr,
        "optimal_threshold": optimal_threshold,
        "optimal_fpr": fpr_optim,
        "optimal_fnr": fnr_optim,
    }
